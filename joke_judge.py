#!/usr/bin/env python3
"""
Joke Judge - Evaluate and Compare Joke Generation Methods

This script evaluates jokes generated by different methods (multi-stage framework vs. baseline)
using an LLM via OpenRouter's API as a judge. It scores jokes on various parameters and compares
the effectiveness of different joke generation approaches.

Usage:
  python joke_judge.py --multistage results.json --baseline baseline_jokes.json
  python joke_judge.py --help
  python joke_judge.py --multistage results.json --baseline baseline.json --api-endpoint "https://openrouter.ai/api/v1"
"""

import argparse
import json
import os
import statistics
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
from openai import OpenAI
from utils.config import get_api_base_url, get_openai_key, get_openrouter_key, JUDGE_MODEL

class JokeJudge:
    def __init__(self, model: str = JUDGE_MODEL, api_endpoint: str = None, api_key: str = None):
        """
        Initialize the joke judge.
        
        Args:
            model: Model to use for judging
            api_endpoint: Custom API endpoint URL (e.g., OpenRouter)
            api_key: API key for the endpoint
        """
        self.model = model
        
        # Use OpenRouter API if endpoint is specified
        if api_endpoint and api_endpoint.strip():
            self.client = OpenAI(
                base_url=api_endpoint,
                api_key=api_key or get_openrouter_key(),
                default_headers={
                    "HTTP-Referer": "https://github.com/joke-generator",  # Required for OpenRouter
                    "X-Title": "Joke Judge"  # Optional for OpenRouter
                }
            )
            print(f"Using custom API endpoint: {api_endpoint}")
        else:
            # Use default endpoint from configuration
            self.client = OpenAI(
                base_url=get_api_base_url(),
                api_key=get_openai_key()
            )
            print(f"Using default API endpoint: {get_api_base_url()}")
            
        self.evaluation_params = [
            "Humor Level", 
            "Originality", 
            "Coherence", 
            "Cleverness", 
            "Appropriateness"
        ]

    def _extract_json_from_text(self, text: str) -> str:
        """
        Extract valid JSON from text response, handling code blocks and other formats.
        
        Args:
            text: Raw text from LLM response
            
        Returns:
            Extracted JSON string
        """
        if not text:
            return "{}"
            
        # Try to find JSON within markdown code blocks
        if "```" in text:
            pattern = r"```(?:json)?\s*([\s\S]*?)```"
            matches = re.findall(pattern, text)
            if matches:
                return matches[0].strip()
        
        # Try to find JSON between curly braces
        try:
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            if (start_idx != -1 and end_idx != -1):
                return text[start_idx:end_idx+1].strip()
        except:
            pass
        
        # Return the original text as a last resort
        return text.strip()

    def load_multistage_jokes(self, filepath: str, filter_fallbacks: bool = True) -> List[Dict[str, Any]]:
        """
        Load jokes from the multi-stage framework results JSON file.
        
        Args:
            filepath: Path to the multi-stage results JSON
            filter_fallbacks: Whether to filter out fallback jokes
            
        Returns:
            List of joke dictionaries with standard format
        """
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            if "jokes" not in data:
                print(f"Error: No 'jokes' field found in {filepath}")
                return []
            
            # Get joke ideas and rubrics for context
            joke_ideas = {idea['id']: idea for idea in data.get("joke_ideas", [])}
            rubrics = {rubric['id']: rubric for rubric in data.get("rubrics", [])}
            
            # Map multi-stage jokes to standard format for judging
            standardized_jokes = []
            filtered_count = 0
            
            for joke in data["jokes"]:
                if "text" in joke:
                    # Filter out fallback jokes if requested
                    if filter_fallbacks and "fallback" in joke["text"].lower():
                        filtered_count += 1
                        continue
                    
                    # Get associated idea and rubric info
                    idea_info = {}
                    if "idea_id" in joke and joke["idea_id"] in joke_ideas:
                        idea = joke_ideas[joke["idea_id"]]
                        idea_info = {"concept": idea.get("concept", "")}
                        
                    rubric_info = {}
                    if "rubric_id" in joke and joke["rubric_id"] in rubrics:
                        rubric = rubrics[joke["rubric_id"]]
                        rubric_info = {
                            "type": rubric.get("type", ""),
                            "structure": rubric.get("structure", ""),
                            "tone": rubric.get("tone", ""),
                            "key_elements": rubric.get("key_elements", [])
                        }
                    
                    standardized_jokes.append({
                        "id": joke.get("id", "unknown"),
                        "text": joke["text"],
                        "method": "multi-stage",
                        "theme": joke.get("theme", "unknown"),
                        "metadata": joke.get("metadata", {}),
                        "explanation": joke.get("explanation", ""),
                        "idea": idea_info,
                        "rubric": rubric_info
                    })
            
            print(f"Loaded {len(standardized_jokes)} jokes from multi-stage results (filtered {filtered_count} fallbacks)")
            return standardized_jokes
        
        except Exception as e:
            print(f"Error loading multi-stage jokes: {e}")
            import traceback
            traceback.print_exc()
            return []

    def load_baseline_jokes(self, filepath: str, filter_fallbacks: bool = True) -> List[Dict[str, Any]]:
        """
        Load jokes from the baseline generator results JSON file.
        
        Args:
            filepath: Path to the baseline results JSON
            filter_fallbacks: Whether to filter out fallback jokes
            
        Returns:
            List of joke dictionaries with standard format
        """
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
                
            if "jokes" not in data:
                print(f"Error: No 'jokes' field found in {filepath}")
                return []
            
            # Map baseline jokes to standard format for judging
            standardized_jokes = []
            filtered_count = 0
            
            for joke in data["jokes"]:
                if "text" in joke:
                    # Filter out fallback jokes if requested
                    if filter_fallbacks and "fallback" in joke["text"].lower():
                        filtered_count += 1
                        continue
                    
                    standardized_jokes.append({
                        "id": joke.get("id", "unknown"),
                        "text": joke["text"],
                        "method": "baseline",
                        "theme": joke.get("prompt", "unknown"),
                        "metadata": {
                            "type": joke.get("type", "General"),
                            "tone": joke.get("tone", "Standard"),
                            "approach": joke.get("approach", "Direct")
                        }
                    })
            
            print(f"Loaded {len(standardized_jokes)} jokes from baseline results (filtered {filtered_count} fallbacks)")
            return standardized_jokes
        
        except Exception as e:
            print(f"Error loading baseline jokes: {e}")
            return []

    def judge_joke(self, joke: Dict[str, Any]) -> Dict[str, Any]:
        """
        Judge a single joke using the LLM, scoring it on various parameters.
        
        Args:
            joke: The joke dictionary to judge
            
        Returns:
            Dictionary with scores and explanation
        """
        try:
            # Construct context information based on method
            context_info = ""
            if joke["method"] == "multi-stage" and "idea" in joke and "rubric" in joke:
                if joke["idea"].get("concept"):
                    context_info += f"\nIdea Concept: {joke['idea']['concept']}"
                
                if joke["rubric"].get("type") and joke["rubric"].get("structure"):
                    context_info += f"\nRubric Type: {joke['rubric']['type']}"
                    context_info += f"\nRubric Structure: {joke['rubric']['structure']}"
                    context_info += f"\nRubric Tone: {joke['rubric']['tone']}"
                    
                    if joke["rubric"].get("key_elements"):
                        elements_str = ", ".join(joke["rubric"]["key_elements"])
                        context_info += f"\nKey Elements: {elements_str}"
            
            prompt = (
                f"As a professional comedy critic, evaluate the following joke objectively:\n\n"
                f"JOKE: \"{joke['text']}\""
                f"{context_info}\n\n"
                f"Please rate this joke on the following parameters (score 1-10 where 10 is best):\n"
                f"- Humor Level: How funny is the joke?\n"
                f"- Originality: How unique/novel is the joke?\n"
                f"- Coherence: How well-structured and logical is the joke?\n"
                f"- Cleverness: How intellectually satisfying is the joke?\n"
                f"- Appropriateness: How suitable is the joke for a general audience?\n\n"
                f"First provide a brief critical analysis of the joke (max 150 words), "
                f"then score each parameter individually, and finally provide an overall score. "
                f"Format your response as valid JSON with keys for 'Analysis' and each parameter name, plus 'Overall'."
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert comedy critic with decades of experience evaluating jokes. Be honest, fair, and precise in your evaluations. Return your analysis and scores in valid JSON format."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
            )
            
            raw_response_content = response.choices[0].message.content
            print(f"Raw LLM Response: {raw_response_content[:200]}...")
            
            # Extract and clean JSON from response
            json_str = self._extract_json_from_text(raw_response_content)
            
            try:
                result = json.loads(json_str)
                
                # Ensure we have all expected fields
                if not all(param in result for param in self.evaluation_params + ["Overall", "Analysis"]):
                    # Try to extract scores using simpler parsing
                    result = self._parse_non_json_response(raw_response_content)
            except json.JSONDecodeError:
                # Failed to parse as JSON, use manual parsing
                result = self._parse_non_json_response(raw_response_content)
            
            judgment = {
                "joke_id": joke["id"],
                "method": joke["method"],
                "text": joke["text"],
                "analysis": result.get("Analysis", "No analysis provided"),
                "scores": {
                    param: result.get(param, 5) for param in self.evaluation_params
                },
                "overall": result.get("Overall", 5)
            }
            
            return judgment
        
        except Exception as e:
            print(f"Error judging joke: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "joke_id": joke["id"],
                "method": joke["method"],
                "text": joke["text"],
                "analysis": "Error during evaluation",
                "scores": {param: 5 for param in self.evaluation_params},
                "overall": 5
            }

    def _parse_non_json_response(self, raw_text: str) -> Dict[str, Any]:
        """
        Parse a non-JSON response from the LLM.
        
        Args:
            raw_text: The raw text from the LLM
            
        Returns:
            Dictionary with extracted scores and analysis
        """
        result = {"Analysis": ""}
        
        lines = raw_text.split('\n')
        in_analysis = False
        
        for line in lines:
            line = line.strip()
            
            # Look for parameter scores
            for param in self.evaluation_params + ["Overall"]:
                if param in line and ":" in line:
                    try:
                        # Try to extract score (last number in the line)
                        import re
                        numbers = re.findall(r'\d+', line)
                        if numbers:
                            result[param] = int(numbers[-1])
                    except (ValueError, IndexError):
                        result[param] = 5
            
            # Collect analysis text
            if "Analysis" in line or "analysis" in line:
                in_analysis = True
                result["Analysis"] = line.replace("Analysis:", "").replace("analysis:", "").strip()
            elif in_analysis and not any(p in line for p in self.evaluation_params + ["Overall"]):
                result["Analysis"] += " " + line
        
        return result

    def judge_all_jokes(self, jokes: List[Dict[str, Any]], output_file: str = None) -> List[Dict[str, Any]]:
        """
        Judge all jokes in the list.
        
        Args:
            jokes: List of joke dictionaries to judge
            output_file: Optional path to save judgments
            
        Returns:
            List of judgment dictionaries
        """
        judgments = []
        
        for i, joke in enumerate(jokes):
            print(f"\nJudging joke {i+1}/{len(jokes)} ({joke['method']}):")
            print(f"  \"{joke['text'][:100]}...\"")
            
            # Print context for multi-stage jokes
            if joke['method'] == "multi-stage" and "idea" in joke and joke["idea"].get("concept"):
                print(f"  Idea: {joke['idea']['concept']}")
                if "rubric" in joke and joke["rubric"].get("type"):
                    print(f"  Rubric: {joke['rubric']['type']} (Tone: {joke['rubric'].get('tone', 'Unknown')})")
            
            judgment = self.judge_joke(joke)
            judgments.append(judgment)
            
            # Print judgment summary
            print(f"  Analysis: {judgment['analysis'][:100]}...")
            print(f"  Overall Score: {judgment['overall']}/10")
        
        if output_file:
            try:
                with open(output_file, 'w') as f:
                    json.dump({"judgments": judgments}, f, indent=2)
                print(f"\nJudgments saved to {output_file}")
            except Exception as e:
                print(f"Error saving judgments: {e}")
        
        return judgments

    def calculate_statistics(self, judgments: List[Dict[str, Any]]) -> Tuple[Dict[str, Dict[str, float]], Dict[str, float]]:
        """
        Calculate statistics for judgments, grouped by method.
        
        Args:
            judgments: List of judgment dictionaries
            
        Returns:
            Tuple of (parameter_stats, overall_stats) dictionaries
        """
        # Group judgments by method
        grouped = {}
        for judgment in judgments:
            method = judgment["method"]
            if method not in grouped:
                grouped[method] = []
            grouped[method].append(judgment)
        
        # Calculate stats for each parameter and method
        parameter_stats = {}
        overall_stats = {}
        
        for method, method_judgments in grouped.items():
            # Calculate overall average
            overall_scores = [j["overall"] for j in method_judgments]
            overall_stats[method] = {
                "mean": statistics.mean(overall_scores),
                "median": statistics.median(overall_scores),
                "stdev": statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0
            }
            
            # Calculate per-parameter stats
            parameter_stats[method] = {}
            for param in self.evaluation_params:
                param_scores = [j["scores"][param] for j in method_judgments]
                parameter_stats[method][param] = {
                    "mean": statistics.mean(param_scores),
                    "median": statistics.median(param_scores),
                    "stdev": statistics.stdev(param_scores) if len(param_scores) > 1 else 0
                }
        
        return parameter_stats, overall_stats

    def print_comparison(self, parameter_stats: Dict[str, Dict[str, float]], overall_stats: Dict[str, float]):
        """
        Print a comparison of different joke generation methods.
        
        Args:
            parameter_stats: Parameter statistics dictionary
            overall_stats: Overall statistics dictionary
        """
        print("\n" + "="*60)
        print("JOKE GENERATION METHOD COMPARISON")
        print("="*60)
        
        methods = list(overall_stats.keys())
        
        # Print overall scores first
        print("\nOVERALL SCORES:")
        for method in methods:
            print(f"  {method.upper()}: {overall_stats[method]['mean']:.2f}/10 " +
                  f"(median: {overall_stats[method]['median']}, stdev: {overall_stats[method]['stdev']:.2f})")
        
        # Print parameter scores
        print("\nPARAMETER SCORES:")
        for param in self.evaluation_params:
            print(f"\n{param}:")
            for method in methods:
                stats = parameter_stats[method][param]
                print(f"  {method.upper()}: {stats['mean']:.2f}/10 " +
                      f"(median: {stats['median']}, stdev: {stats['stdev']:.2f})")
        
        # Determine winner - Only compare when there are exactly 2 methods
        if len(methods) == 2:
            # Get the two methods
            method1, method2 = methods
            
            # Calculate the margin
            margin = abs(overall_stats[method1]['mean'] - overall_stats[method2]['mean'])
            
            # Determine which method scored higher
            best_method = method1 if overall_stats[method1]['mean'] > overall_stats[method2]['mean'] else method2
            other_method = method2 if best_method == method1 else method1
            
            print("\n" + "-"*60)
            if margin > 0.5:
                print(f"CONCLUSION: {best_method.upper()} jokes scored higher by {margin:.2f} points on average.")
            else:
                print(f"CONCLUSION: Both approaches performed similarly (margin: {margin:.2f}).")
        elif len(methods) > 2:
            # Handle comparison with more than 2 methods
            print("\n" + "-"*60)
            print("CONCLUSION: Multiple methods compared. Overall score ranking:")
            
            # Sort methods by mean score in descending order
            sorted_methods = sorted(methods, key=lambda m: overall_stats[m]['mean'], reverse=True)
            for i, method in enumerate(sorted_methods):
                print(f"  {i+1}. {method.upper()}: {overall_stats[method]['mean']:.2f}/10")
        
        print("="*60)

def main():
    """Main function to handle CLI arguments and run the joke judge"""
    from utils.config import initialize_config
    
    if not initialize_config():
        print("Failed to initialize configuration. Please check your .env file.")
        return
    
    parser = argparse.ArgumentParser(description="Judge and compare jokes from different generation methods")
    parser.add_argument("--multistage", help="Path to multi-stage framework results JSON")
    parser.add_argument("--baseline", help="Path to baseline generator results JSON")
    parser.add_argument("--output", default="joke_judgments.json", help="Output file for judgments")
    parser.add_argument("--model", default=JUDGE_MODEL, help="Model to use for judging")
    parser.add_argument("--samples", type=int, default=0, help="Number of jokes to sample from each method (0 for all)")
    parser.add_argument("--api-endpoint", help="Custom API endpoint URL (e.g., OpenRouter)")
    parser.add_argument("--api-key", help="API key for the endpoint (or set OPENROUTER_API_KEY env var)")
    parser.add_argument("--include-fallbacks", action="store_true", help="Include fallback jokes in evaluation")
    
    args = parser.parse_args()
    
    # Check for API keys
    if args.api_endpoint and "openrouter" in args.api_endpoint:
        api_key = args.api_key or get_openrouter_key()
        if not api_key:
            print("Warning: OPENROUTER_API_KEY not found in configuration or command line arguments")
            return
    elif not get_openai_key():
        print("Warning: OPENAI_API_KEY not found in configuration")
        return
    
    if not args.multistage and not args.baseline:
        print("Error: Please provide at least one of --multistage or --baseline")
        return
    
    # Initialize judge with custom API endpoint if provided
    judge = JokeJudge(
        model=args.model, 
        api_endpoint=args.api_endpoint,
        api_key=args.api_key
    )
    
    all_jokes = []
    
    # Load jokes, filtering out fallback jokes unless explicitly included
    filter_fallbacks = not args.include_fallbacks
    
    if args.multistage:
        multistage_jokes = judge.load_multistage_jokes(args.multistage, filter_fallbacks=filter_fallbacks)
        if args.samples > 0 and len(multistage_jokes) > args.samples:
            import random
            multistage_jokes = random.sample(multistage_jokes, args.samples)
        all_jokes.extend(multistage_jokes)
    
    if args.baseline:
        baseline_jokes = judge.load_baseline_jokes(args.baseline, filter_fallbacks=filter_fallbacks)
        if args.samples > 0 and len(baseline_jokes) > args.samples:
            import random
            baseline_jokes = random.sample(baseline_jokes, args.samples)
        all_jokes.extend(baseline_jokes)
    
    if not all_jokes:
        print("Error: No jokes loaded for judging")
        return
    
    # Judge all jokes
    judgments = judge.judge_all_jokes(all_jokes, args.output)
    
    # Calculate and print statistics
    parameter_stats, overall_stats = judge.calculate_statistics(judgments)
    judge.print_comparison(parameter_stats, overall_stats)


if __name__ == "__main__":
    main()
